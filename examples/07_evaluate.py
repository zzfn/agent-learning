"""
DSPy Evaluateï¼ˆè¯„ä¼°ç³»ç»Ÿï¼‰ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ç³»ç»ŸåŒ–åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½
"""

import dspy
import os
from dotenv import load_dotenv
from dspy.evaluate import Evaluate

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def main():
    # é…ç½®è¯­è¨€æ¨¡å‹ - ä½¿ç”¨ DeepSeek
    lm = dspy.LM(
        'deepseek/deepseek-chat',
        api_key=os.getenv('DEEPSEEK_API_KEY')
    )
    dspy.configure(lm=lm)

    print("=" * 70)
    print("DSPy Evaluate: ç³»ç»ŸåŒ–è¯„ä¼°")
    print("=" * 70)

    # ç¤ºä¾‹ 1: åŸºæœ¬è¯„ä¼° - æƒ…æ„Ÿåˆ†ç±»
    print("\nğŸ“‹ ç¤ºä¾‹ 1: åŸºæœ¬è¯„ä¼°æµç¨‹")
    print("-" * 70)

    # å®šä¹‰ä»»åŠ¡
    class SentimentClassification(dspy.Signature):
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        text = dspy.InputField(desc="è¦åˆ†æçš„æ–‡æœ¬")
        sentiment = dspy.OutputField(desc="æƒ…æ„Ÿï¼šç§¯æã€æ¶ˆæã€ä¸­æ€§")

    # åˆ›å»ºæ¨¡å‹
    sentiment_model = dspy.Predict(SentimentClassification)

    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_set = [
        dspy.Example(
            text="è¿™ä¸ªäº§å“è´¨é‡éå¸¸å¥½ï¼Œæˆ‘å¾ˆæ»¡æ„ï¼",
            sentiment="ç§¯æ"
        ).with_inputs("text"),
        dspy.Example(
            text="å¤ªå¤±æœ›äº†ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·æ ¼ã€‚",
            sentiment="æ¶ˆæ"
        ).with_inputs("text"),
        dspy.Example(
            text="è¿˜å¯ä»¥ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„ã€‚",
            sentiment="ä¸­æ€§"
        ).with_inputs("text"),
        dspy.Example(
            text="æœåŠ¡æ€åº¦å¾ˆå¥½ï¼Œç‰©æµä¹Ÿå¾ˆå¿«ï¼",
            sentiment="ç§¯æ"
        ).with_inputs("text"),
        dspy.Example(
            text="è´¨é‡å¤ªå·®äº†ï¼Œä¸æ¨èè´­ä¹°ã€‚",
            sentiment="æ¶ˆæ"
        ).with_inputs("text"),
    ]

    print(f"\nå‡†å¤‡äº† {len(test_set)} æ¡æµ‹è¯•æ•°æ®")

    # å®šä¹‰è¯„ä¼°æŒ‡æ ‡
    def accuracy_metric(example, pred, trace=None):
        """è®¡ç®—å‡†ç¡®ç‡"""
        return example.sentiment.strip() == pred.sentiment.strip()

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = Evaluate(
        devset=test_set,
        metric=accuracy_metric,
        num_threads=1,  # å•çº¿ç¨‹æ‰§è¡Œ
        display_progress=True,
        display_table=5  # æ˜¾ç¤ºå‰5æ¡ç»“æœ
    )

    # è¿è¡Œè¯„ä¼°
    print("\nå¼€å§‹è¯„ä¼°...")
    result = evaluator(sentiment_model)
    print(f"\nâœ“ å‡†ç¡®ç‡: {result.score:.1%}")

    # ç¤ºä¾‹ 2: å¤šæŒ‡æ ‡è¯„ä¼°
    print("\n\nğŸ“‹ ç¤ºä¾‹ 2: å¤šæŒ‡æ ‡è¯„ä¼°")
    print("-" * 70)

    class QuestionAnswer(dspy.Signature):
        """å›ç­”é—®é¢˜"""
        question = dspy.InputField(desc="é—®é¢˜")
        answer = dspy.OutputField(desc="ç­”æ¡ˆ")

    qa_model = dspy.ChainOfThought(QuestionAnswer)

    # æµ‹è¯•æ•°æ®
    qa_test_set = [
        dspy.Example(
            question="Pythonæ˜¯ä»€ä¹ˆæ—¶å€™å‘å¸ƒçš„ï¼Ÿ",
            answer="1991å¹´"
        ).with_inputs("question"),
        dspy.Example(
            question="è°åˆ›å»ºäº†Pythonï¼Ÿ",
            answer="Guido van Rossum"
        ).with_inputs("question"),
        dspy.Example(
            question="DSPyæ˜¯å“ªä¸ªå¤§å­¦å¼€å‘çš„ï¼Ÿ",
            answer="æ–¯å¦ç¦å¤§å­¦"
        ).with_inputs("question"),
    ]

    # å®šä¹‰å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
    def exact_match(example, pred, trace=None):
        """ç²¾ç¡®åŒ¹é…"""
        return example.answer.strip() in pred.answer.strip()

    def has_key_info(example, pred, trace=None):
        """åŒ…å«å…³é”®ä¿¡æ¯"""
        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«å…³é”®è¯
        key_words = example.answer.strip().split()
        return any(word in pred.answer for word in key_words)

    def answer_length_check(example, pred, trace=None):
        """ç­”æ¡ˆé•¿åº¦æ£€æŸ¥ï¼ˆä¸è¦å¤ªé•¿ï¼‰"""
        return len(pred.answer) <= 200

    # åˆ†åˆ«è¯„ä¼°
    print("\nè¯„ä¼°æŒ‡æ ‡ 1: ç²¾ç¡®åŒ¹é…")
    evaluator1 = Evaluate(
        devset=qa_test_set,
        metric=exact_match,
        num_threads=1,
    )
    result1 = evaluator1(qa_model)
    print(f"ç²¾ç¡®åŒ¹é…å¾—åˆ†: {result1.score:.1%}")

    print("\nè¯„ä¼°æŒ‡æ ‡ 2: åŒ…å«å…³é”®ä¿¡æ¯")
    evaluator2 = Evaluate(
        devset=qa_test_set,
        metric=has_key_info,
        num_threads=1,
    )
    result2 = evaluator2(qa_model)
    print(f"å…³é”®ä¿¡æ¯å¾—åˆ†: {result2.score:.1%}")

    print("\nè¯„ä¼°æŒ‡æ ‡ 3: ç­”æ¡ˆé•¿åº¦æ£€æŸ¥")
    evaluator3 = Evaluate(
        devset=qa_test_set,
        metric=answer_length_check,
        num_threads=1,
    )
    result3 = evaluator3(qa_model)
    print(f"é•¿åº¦æ£€æŸ¥å¾—åˆ†: {result3.score:.1%}")

    # ç¤ºä¾‹ 3: è¯„ä¼°ä¼˜åŒ–å‰åçš„æ€§èƒ½
    print("\n\nğŸ“‹ ç¤ºä¾‹ 3: å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½")
    print("-" * 70)

    # å‡†å¤‡æ›´å¤šè®­ç»ƒæ•°æ®
    trainset = [
        dspy.Example(
            text="è¿™å®¶é¤å…å¤ªæ£’äº†ï¼",
            sentiment="ç§¯æ"
        ).with_inputs("text"),
        dspy.Example(
            text="æœåŠ¡å¤ªå·®äº†ã€‚",
            sentiment="æ¶ˆæ"
        ).with_inputs("text"),
        dspy.Example(
            text="ä¸€èˆ¬èˆ¬ã€‚",
            sentiment="ä¸­æ€§"
        ).with_inputs("text"),
        dspy.Example(
            text="è¶…çº§å–œæ¬¢ï¼",
            sentiment="ç§¯æ"
        ).with_inputs("text"),
        dspy.Example(
            text="éå¸¸ç³Ÿç³•ã€‚",
            sentiment="æ¶ˆæ"
        ).with_inputs("text"),
    ]

    # æœªä¼˜åŒ–çš„æ¨¡å‹
    unoptimized = dspy.Predict(SentimentClassification)

    # ä½¿ç”¨ BootstrapFewShot ä¼˜åŒ–
    from dspy.teleprompt import BootstrapFewShot

    optimizer = BootstrapFewShot(
        metric=accuracy_metric,
        max_bootstrapped_demos=2,
    )

    print("\næ­£åœ¨ä¼˜åŒ–æ¨¡å‹...")
    optimized = optimizer.compile(
        dspy.Predict(SentimentClassification),
        trainset=trainset
    )
    print("âœ“ ä¼˜åŒ–å®Œæˆ")

    # è¯„ä¼°æœªä¼˜åŒ–æ¨¡å‹
    print("\nè¯„ä¼°æœªä¼˜åŒ–æ¨¡å‹...")
    evaluator = Evaluate(
        devset=test_set,
        metric=accuracy_metric,
        num_threads=1,
    )
    result_before = evaluator(unoptimized)
    score_before = result_before.score
    print(f"æœªä¼˜åŒ–æ¨¡å‹å‡†ç¡®ç‡: {score_before:.1%}")

    # è¯„ä¼°ä¼˜åŒ–åæ¨¡å‹
    print("\nè¯„ä¼°ä¼˜åŒ–åæ¨¡å‹...")
    result_after = evaluator(optimized)
    score_after = result_after.score
    print(f"ä¼˜åŒ–åæ¨¡å‹å‡†ç¡®ç‡: {score_after:.1%}")

    # æ€§èƒ½æå‡
    improvement = score_after - score_before
    print(f"\næ€§èƒ½æå‡: {improvement:+.1%}")

    # ç¤ºä¾‹ 4: è‡ªå®šä¹‰å¤æ‚è¯„ä¼°æŒ‡æ ‡
    print("\n\nğŸ“‹ ç¤ºä¾‹ 4: è‡ªå®šä¹‰å¤æ‚è¯„ä¼°æŒ‡æ ‡")
    print("-" * 70)

    class TranslationTask(dspy.Signature):
        """ç¿»è¯‘ä»»åŠ¡"""
        text = dspy.InputField(desc="è‹±æ–‡æ–‡æœ¬")
        translation = dspy.OutputField(desc="ä¸­æ–‡ç¿»è¯‘")

    translator = dspy.Predict(TranslationTask)

    translation_test = [
        dspy.Example(
            text="Hello, how are you?",
            translation="ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ"
        ).with_inputs("text"),
        dspy.Example(
            text="Good morning!",
            translation="æ—©ä¸Šå¥½ï¼"
        ).with_inputs("text"),
    ]

    def translation_quality(example, pred, trace=None):
        """
        å¤æ‚çš„ç¿»è¯‘è´¨é‡è¯„ä¼°
        è€ƒè™‘å¤šä¸ªå› ç´ ï¼šé•¿åº¦ã€å…³é”®è¯ã€æµç•…æ€§
        """
        score = 0.0

        # å› ç´ 1: é•¿åº¦åˆç†æ€§ï¼ˆ0-0.3åˆ†ï¼‰
        expected_len = len(example.translation)
        actual_len = len(pred.translation)
        len_ratio = min(actual_len, expected_len) / max(actual_len, expected_len)
        score += len_ratio * 0.3

        # å› ç´ 2: åŒ…å«å…³é”®æ¦‚å¿µï¼ˆ0-0.4åˆ†ï¼‰
        # ç®€åŒ–ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯éç©ºç¿»è¯‘
        if pred.translation and len(pred.translation) > 0:
            score += 0.4

        # å› ç´ 3: åŸºæœ¬æ­£ç¡®æ€§ï¼ˆ0-0.3åˆ†ï¼‰
        # ç®€åŒ–ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        if any('\u4e00' <= char <= '\u9fff' for char in pred.translation):
            score += 0.3

        return score

    print("\nè¯„ä¼°ç¿»è¯‘è´¨é‡ï¼ˆè‡ªå®šä¹‰å¤æ‚æŒ‡æ ‡ï¼‰...")
    evaluator = Evaluate(
        devset=translation_test,
        metric=translation_quality,
        num_threads=1,
    )
    result = evaluator(translator)
    print(f"ç¿»è¯‘è´¨é‡å¾—åˆ†: {result.score:.1%}")

    # è¯´æ˜
    print("\n\n" + "=" * 70)
    print("ğŸ’¡ Evaluate ç³»ç»Ÿçš„ç‰¹ç‚¹")
    print("=" * 70)
    print("""
DSPy Evaluate çš„æ ¸å¿ƒåŠŸèƒ½:

1. **ç³»ç»ŸåŒ–è¯„ä¼°**
   - æ ‡å‡†åŒ–çš„è¯„ä¼°æµç¨‹
   - æ”¯æŒæ‰¹é‡æµ‹è¯•æ•°æ®
   - è‡ªåŠ¨è®¡ç®—èšåˆæŒ‡æ ‡

2. **è¯„ä¼°æŒ‡æ ‡ (Metric)**
   - å‡½æ•°ç­¾å: metric(example, pred, trace=None) -> score
   - è¿”å›å€¼å¯ä»¥æ˜¯å¸ƒå°”å€¼ï¼ˆTrue/Falseï¼‰æˆ–æµ®ç‚¹æ•°ï¼ˆ0.0-1.0ï¼‰
   - å¯ä»¥è‡ªå®šä¹‰å¤æ‚çš„è¯„ä¼°é€»è¾‘

3. **å¸¸è§è¯„ä¼°æŒ‡æ ‡ç±»å‹**
   - å‡†ç¡®ç‡ (Accuracy): ç²¾ç¡®åŒ¹é…
   - åŒ…å«æ£€æŸ¥: æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
   - æ ¼å¼æ£€æŸ¥: è¾“å‡ºæ ¼å¼æ˜¯å¦æ­£ç¡®
   - ç›¸ä¼¼åº¦: è¯­ä¹‰ç›¸ä¼¼åº¦ã€ç¼–è¾‘è·ç¦»ç­‰
   - å¤åˆæŒ‡æ ‡: å¤šä¸ªå› ç´ åŠ æƒ

4. **Evaluate å‚æ•°å’Œè¿”å›å€¼**
   å‚æ•°:
   - devset: æµ‹è¯•æ•°æ®é›†
   - metric: è¯„ä¼°æŒ‡æ ‡å‡½æ•°
   - num_threads: å¹¶è¡Œçº¿ç¨‹æ•°
   - display_progress: æ˜¾ç¤ºè¿›åº¦æ¡
   - display_table: æ˜¾ç¤ºç»“æœè¡¨æ ¼

   è¿”å›å€¼ (EvaluationResult):
   - score: å¹³å‡å¾—åˆ†ï¼ˆæµ®ç‚¹æ•° 0.0-1.0ï¼‰
   - results: æ‰€æœ‰æ ·æœ¬çš„è¯¦ç»†ç»“æœåˆ—è¡¨

5. **å®é™…åº”ç”¨åœºæ™¯**
   - æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•
   - ä¼˜åŒ–å‰åæ€§èƒ½å¯¹æ¯”
   - A/B æµ‹è¯•ä¸åŒæ¨¡å‹
   - æŒç»­ç›‘æ§æ¨¡å‹æ€§èƒ½
   - å‘ç°æ¨¡å‹å¼±ç‚¹

6. **æœ€ä½³å®è·µ**
   - å‡†å¤‡é«˜è´¨é‡çš„æµ‹è¯•é›†
   - å®šä¹‰æ¸…æ™°çš„è¯„ä¼°æ ‡å‡†
   - ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡ç»¼åˆè¯„ä¼°
   - å®šæœŸè¯„ä¼°æ¨¡å‹æ€§èƒ½
   - è®°å½•è¯„ä¼°ç»“æœç”¨äºè¿½è¸ª

7. **è¯„ä¼° + ä¼˜åŒ–çš„å®Œæ•´æµç¨‹**
   ```
   1. å‡†å¤‡æ•°æ®é›†ï¼ˆè®­ç»ƒé›† + æµ‹è¯•é›†ï¼‰
   2. å®šä¹‰è¯„ä¼°æŒ‡æ ‡
   3. è¯„ä¼°åŸºç¡€æ¨¡å‹æ€§èƒ½
   4. ä½¿ç”¨ä¼˜åŒ–å™¨ä¼˜åŒ–æ¨¡å‹
   5. è¯„ä¼°ä¼˜åŒ–åæ€§èƒ½
   6. å¯¹æ¯”åˆ†æç»“æœ
   7. è¿­ä»£æ”¹è¿›
   ```
    """)

if __name__ == "__main__":
    main()
